#!/usr/bin/env python3
"""Tests de la cha√Æne de d√©l√©gation compl√®te"""

import pytest
import time
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from main import ALMAA
from core.base import Message
from agents.special.philosophe import AgentPhilosophe
from loguru import logger

# Configuration pour les tests
logger.remove()
logger.add(sys.stdout, level="INFO")

class TestFullDelegation:
    """Tests de la d√©l√©gation compl√®te User ‚Üí Chef ‚Üí ChefProjet ‚Üí Worker ‚Üí User"""

    def setup_method(self):
        """Setup pour chaque test"""
        self.almaa = ALMAA()

        # Ajouter le Philosophe pour observer
        philosophe = AgentPhilosophe()
        self.almaa.register_agent(philosophe)

        # Hook pour tracer tous les messages
        self.message_trace = []
        original_publish = self.almaa.bus.publish

        def traced_publish(message):
            self.message_trace.append({
                "sender": message.sender,
                "recipient": message.recipient,
                "type": message.type,
                "timestamp": time.time()
            })
            logger.debug(f"üì® {message.sender} ‚Üí {message.recipient} ({message.type})")
            return original_publish(message)

        self.almaa.bus.publish = traced_publish

    def teardown_method(self):
        """Cleanup apr√®s chaque test"""
        self.almaa.shutdown()

    def test_simple_code_generation(self):
        """Test : g√©n√©ration de code simple"""

        # Requ√™te utilisateur
        result = self.almaa.process_request(
            "Cr√©e une fonction Python pour calculer la factorielle",
            timeout=20
        )

        # V√©rifications
        assert result['success'], f"√âchec: {result.get('error')}"
        assert "def" in result['response'], "Pas de code Python dans la r√©ponse"
        assert "factorial" in result['response'].lower() or "factorielle" in result['response'].lower()

        # V√©rifier la cha√Æne de d√©l√©gation
        self._verify_delegation_chain()

    def test_complex_task_delegation(self):
        """Test : t√¢che complexe n√©cessitant planification"""

        result = self.almaa.process_request(
            "Cr√©e une API REST compl√®te avec FastAPI pour g√©rer des todos",
            timeout=30
        )

        assert result['success']

        # V√©rifier que ChefProjet a √©t√© impliqu√©
        chef_projet_messages = [
            m for m in self.message_trace 
            if m['sender'] == 'ChefProjet' or m['recipient'] == 'ChefProjet'
        ]
        assert len(chef_projet_messages) > 0, "ChefProjet n'a pas √©t√© impliqu√©"

    def test_error_handling(self):
        """Test : gestion des erreurs"""

        # Requ√™te impossible
        result = self.almaa.process_request(
            "T√ÇCHE_IMPOSSIBLE_12345_!@#$%",
            timeout=10
        )

        # Devrait retourner une r√©ponse, m√™me en cas d'erreur
        assert 'response' in result or 'error' in result

    def test_philosophe_observation(self):
        """Test : le Philosophe observe correctement"""

        # Faire quelques requ√™tes
        for i in range(3):
            self.almaa.process_request(f"Test {i}", timeout=5)

        # Demander un rapport au Philosophe
        report_request = Message(
            sender="Test",
            recipient="Philosophe",
            type="REPORT_REQUEST",
            content={}
        )

        self.almaa.bus.publish(report_request)
        self.almaa.bus.process_agent_messages()

        # V√©rifier que le Philosophe a observ√©
        philosophe = self.almaa.agents.get("Philosophe")
        assert philosophe is not None
        assert philosophe.metrics["total_messages"] > 0

        # G√©n√©rer rapport
        report = philosophe.generate_report()
        assert report["period"]["total_messages"] > 0
        assert len(report["message_distribution"]) > 0

    def test_parallel_requests(self):
        """Test : requ√™tes parall√®les"""

        import threading
        results = []

        def make_request(request_text):
            result = self.almaa.process_request(request_text, timeout=15)
            results.append(result)

        # Lancer 3 requ√™tes en parall√®le
        threads = []
        requests = [
            "Cr√©e une fonction pour trier une liste",
            "G√©n√®re une classe Person avec nom et √¢ge",
            "√âcris un script pour lire un fichier CSV"
        ]

        for req in requests:
            t = threading.Thread(target=make_request, args=(req,))
            threads.append(t)
            t.start()

        # Attendre que toutes finissent
        for t in threads:
            t.join()

        # V√©rifier les r√©sultats
        assert len(results) == 3
        success_count = sum(1 for r in results if r.get('success', False))
        assert success_count >= 2, f"Seulement {success_count}/3 requ√™tes r√©ussies"

    def _verify_delegation_chain(self):
        """V√©rifie que la cha√Æne de d√©l√©gation est correcte"""

        # Extraire la s√©quence des messages
        chain = []
        for msg in self.message_trace:
            chain.append(f"{msg['sender']}‚Üí{msg['recipient']}")

        logger.info(f"Cha√Æne de d√©l√©gation: {' | '.join(chain[:10])}")

        # V√©rifications de base
        assert any("User‚ÜíChef" in c for c in chain), "User n'a pas contact√© Chef"
        assert any("Chef‚Üí" in c for c in chain), "Chef n'a pas d√©l√©gu√©"

def test_integration_scenarios():
    """Tests de sc√©narios d'int√©gration complets"""

    scenarios = [
        {
            "name": "D√©veloppement simple",
            "request": "Cr√©e une fonction pour v√©rifier si un nombre est premier",
            "expected_in_response": ["def", "prime", "return"],
            "max_time": 15
        },
        {
            "name": "Analyse de code",
            "request": "Analyse ce code: def add(a, b): return a + b",
            "expected_in_response": ["fonction", "addition", "simple"],
            "max_time": 10
        },
        {
            "name": "Documentation",
            "request": "G√©n√®re la documentation pour une fonction de tri",
            "expected_in_response": ["Args:", "Returns:", "Example:"],
            "max_time": 12
        }
    ]

    almaa = ALMAA()
    failed = []

    for scenario in scenarios:
        logger.info(f"\nüß™ Test: {scenario['name']}")

        start = time.time()
        result = almaa.process_request(scenario["request"], timeout=scenario["max_time"])
        duration = time.time() - start

        if result.get('success'):
            response = result.get('response', '').lower()

            # V√©rifier les √©l√©ments attendus
            missing = []
            for expected in scenario["expected_in_response"]:
                if expected.lower() not in response:
                    missing.append(expected)

            if missing:
                logger.warning(f"‚ö†Ô∏è  √âl√©ments manquants: {missing}")
                failed.append(scenario["name"])
            else:
                logger.success(f"‚úÖ Succ√®s en {duration:.1f}s")
        else:
            logger.error(f"‚ùå √âchec: {result.get('error')}")
            failed.append(scenario["name"])

    almaa.shutdown()

    # R√©sum√©
    logger.info(f"\nüìä R√©sum√©: {len(scenarios) - len(failed)}/{len(scenarios)} r√©ussis")

    if failed:
        logger.error(f"√âchecs: {', '.join(failed)}")

    return len(failed) == 0

if __name__ == "__main__":
    # Lancer les tests
    logger.info("üöÄ Tests de d√©l√©gation compl√®te ALMAA\n")

    # Tests unitaires avec pytest
    pytest.main([__file__, "-v", "--tb=short"])

    # Tests d'int√©gration
    logger.info("\n" + "="*60)
    logger.info("üß™ TESTS D'INT√âGRATION")
    logger.info("="*60)

    success = test_integration_scenarios()

    if success:
        logger.success("\n‚úÖ Tous les tests sont pass√©s!")
    else:
        logger.error("\n‚ùå Certains tests ont √©chou√©")
        sys.exit(1)
